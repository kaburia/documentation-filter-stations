<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Baseline Models</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <style>
        body {
            font-size: 15px;
            line-height: 1.8;
        }
    </style>
</head>

<body>

    <div class="container">

        <div class="row">
            <div class="col-md-9 offset-md-2" style="background-color: rgb(255, 255, 255);">
                <div class="row mt-2">
                    <div class="col-md-12 text-center">
                        <a href="https://dekut-dsail.github.io/">
                            <img class="logo" src="https://dekut-dsail.github.io/assets/img/newLogoPng.png" alt="Logo"
                                height="70px" width="auto">
                        </a>
                    </div>
                </div>
                <div class="row mt-2">
                    <div class="col-md-12 text-center">
                        <h2>Deploying Machine Learning Models on Embedded Systems</h2>
                    </div>
                </div>

                <hr>
                <div class="row">
                    <div class="col-md-12">
                        <nav class="navbar navbar-expand-md navbar-light justify-content-end">
                            <ul class="navbar-nav lead" style="font-size:large;">
                                <li class="nav-item">
                                    <a class="nav-link" href="index.html">Introduction</a>
                                </li>
                                <li class="nav-item">
                                    <a class="nav-link" href="instructors.html">Instructors</a>
                                </li>
                                <li class="nav-item">
                                    <a class="nav-link" href="hardware.html">Hardware</a>
                                </li>
                                <li class="nav-item">
                                    <a class="nav-link" href="setup.html">Setup</a>
                                </li>

                                <li class="nav-item">
                                    <a class="nav-link active" href="image_classification.html">Baseline Model</a>
                                </li>
                                <li class="nav-item">
                                    <a class="nav-link" href="object_detection.html">Camera Trap Application</a>
                                </li>
                                <li class="nav-item">
                                    <a class="nav-link" href="conclusion.html">Conclusion</a>
                                </li>
                            </ul>
                        </nav>
                    </div>
                </div>
                <hr>


                <section id="hardware" class="section">
                    <h4 class="section-title mb-3 text-center">Image Classification Task</h4>
                    <h5>Overview</h5>
                    <p>
                        Image classification refers to the process of assigning a label or class to an input image based
                        on its visual
                        content. It is a fundamental
                        task in computer vision and machine learning. In image classification, a machine learning model
                        analyzes the features
                        and patterns present in an image to predict the most appropriate class or category that the
                        image belongs to. The model
                        is typically trained on a labelled image dataset, learning to recognize and differentiate
                        between different objects,
                        scenes, or concepts.
                    </p>
                    <h5>Procedure</h5>
                    <p>
                        Login to Edge impulse and create a new project “tech4wildlife-base-classification”

                        This will take you to the project page/dashboard where you choose to add existing Data.
                        From the cloned dsail-tech4wildlife repository navigate to the “base-data” folder then choose
                        all the files in the
                        “classification folder”

                        Be sure to infer from the filename and edge impulse will split the data accordingly
                    <div class="row">
                        <div class="col-md-12 text-center">
                            <img src="https://drive.google.com/uc?export=view&id=121wkIiD5dg4xQyPz4S8JnS8Zi0tcYL1v"
                                alt="" height="auto" width="auto">
                            <p class="text-center" style="color: #888;">
                                <i>You should see a page similar to this one</i>
                            </p>
                        </div>
                    </div>
                    <b>Superb!</b>
                    </p>
                    <p>
                    <h5>Let's Create an Impulse</h5>
                    Impulse Design is a component of Edge Impulse that enables you to design machine learning models. It
                    offers a
                    user-friendly interface for defining inputs and outputs, selecting algorithms, and configuring model
                    architecture. It
                    supports classical and deep learning techniques, provides data collection and preprocessing tools,
                    and handles training
                    with performance monitoring. Users can evaluate models using validation data and deploy optimized
                    code to
                    microcontrollers or edge devices. Impulse Design simplifies the process of creating, training, and
                    deploying machine
                    learning models, making them accessible for a wide range of applications.
                    <div class="row">
                        <div class="col-md-12 text-center">
                            <img src="https://drive.google.com/uc?export=view&id=1penJ4SK5-XxKqPbXwGyhIyl6r5a8lSgF"
                                alt="" height="auto" width="auto">
                            <p class="text-center" style="color: #888;">
                                <i>The pipeline to train the images</i>
                            </p>
                        </div>
                    </div>

                    <ul>
                        <li>
                            <b>Image data:</b> To begin, the images will be resized to a dimension of <i>96x96</i>.
                        </li>
                        <li>
                            <b>Image:</b> The image data will then be preprocessed and normalized, with the additional
                            option of reducing the colour depth.
                            Normalizing the image data before training offers the benefit of standardizing the pixel
                            values across the dataset,
                            ensuring consistent and optimal learning conditions for the model. This normalization
                            process aids in mitigating the
                            influence of varying pixel intensity ranges, enabling more effective and reliable training
                            results.
                        </li>
                        <li>
                            <b>Transfer learning:</b> This is a beneficial approach in scenarios where data is scarce
                            and deep neural networks require
                            large amounts of data for effective feature learning. It involves utilizing the knowledge
                            acquired from training a model
                            on one task and applying it to a related but different task. By leveraging pre-trained
                            models as a foundation, transfer
                            learning enables faster and more precise learning in new domains where labelled data is
                            limited.
                        </li>
                        <li>
                            <b>Output features:</b> the classes the model can predict. In this particular case, the
                            model is designed to classify objects
                            into two classes: "bottle" and "computer". These classes represent the two categories that
                            the model has been trained to
                            distinguish between.

                        </li>
                    </ul>
                    </p>
                    <p>
                    <h5>Generating Features</h5>
                    In this step, we save the parameters and generate features for the image while considering the
                    colour depth, which in
                    our case, is RGB. By saving the parameters, we store the learned information from the model,
                    facilitating its
                    reusability or further analysis. Generating features involves extracting meaningful representations
                    from the images and
                    capturing their distinctive characteristics.
                    <div class="row">
                        <div class="col-md-12 text-center">
                            <img src="https://drive.google.com/uc?export=view&id=1Y41aQkCDiZdFlCs1yRJ6zGYB8Tz5nzq_"
                                alt="" height="auto" width="auto">
                            <p class="text-center" style="color: #888;">
                                <i>Navigate to Image under Impulse design</i>
                            </p>
                        </div>
                    </div>

                    <h5>Model training</h5>

                    In this step, we will utilize the MobileNetV1 model with an input image size of 96x96 and a dropout
                    rate of 0.2. This
                    configuration aims to enhance the model's capacity to capture intricate features and prevent
                    overfitting. We will train
                    the model for 30 cycles, adjusting the model's parameters through a learning rate of 0.0005. This
                    iterative training
                    process allows the model to gradually optimize its performance and improve its ability to classify
                    images accurately.
                    <div class="row">
                        <div class="col-md-12 text-center">
                            <img src="https://drive.google.com/uc?export=view&id=12bBcbMuj9iCTCeaUUlKMlo_byFDga108"
                                alt="" height="auto" width="auto">
                            <p class="text-center" style="color: #888;">
                                <i>Model training</i>
                            </p>
                        </div>
                    </div>

                    <h5>Model Evaluation</h5>
                    To assess the model's accuracy, go to the Navigation bar on the left and select the "Model Testing"
                    section.
                    Once there, choose the "Classify All" option to execute the test.
                    After running the test, the results indicate that the model achieved an accuracy of 93.5%. This high
                    accuracy indicates
                    that the model performed well in classifying the test data and is a promising sign of its
                    effectiveness in real-world
                    scenarios.
                    <div class="row">
                        <div class="col-md-12 text-center">
                            <img src="https://drive.google.com/uc?export=view&id=1YvULe7PwkkB6WpBz8knOyVbJvP0dXAPU"
                                alt="" height="auto" width="auto">
                            <p class="text-center" style="color: #888;">
                                <i>The model has performed quite well with 90.5% on the validation set.</i>
                            </p>
                        </div>
                    </div>
                    <div class="row">
                        <div class="col-md-12 text-center">
                            <img src="https://drive.google.com/uc?export=view&id=117THvX0abdeYmrnqT-EO7Im25tPMUrq7"
                                alt="" height="400px" width="700px">
                            <p class="text-center" style="color: #888;">
                                <i>The model accuracy of 93.55% on test set</i>
                            </p>
                        </div>
                    </div>
                    <h5>Model Deployment</h5>
                    To proceed with the deployment:
                    <ul>


                        <li>Navigate to the <code> "Deployment"</code> section.</li>
                        <li>Select the Arduino Nano BLE 33 Sense as the designated board for deployment.</li>
                        <li>Click on the <code>"Build"</code> button to initiate the firmware-building process.</li>
                        <li>After unzipping the generated zip file, select the appropriate file based on your operating
                            system:</li>
                        <ul>
                            <li>For Linux: Use the <code>.sh file</code>.</li>
                            <li>For mac OS: Use the <code>.command file</code>.</li>
                            <li>For Windows: Use the <code>.bat file</code>.</li>
                        </ul>
                        <li>Ensure that the Arduino Nano BLE 33 Sense board is connected to your computer.</li>
                        <li>Double-click on the corresponding file to flash the firmware onto the connected Arduino Nano
                            BLE 33 Sense board.</li>
                        <li>Open your terminal or command prompt. <br>
                            Type the command <code> "edge-impulse-run-impulse --debug" </code>. <br>
                            The command will provide a URL <br>
                            Copy and paste the URL into the browser of your choice.</li>

                    </ul>
                    <br>
                    You should now be able to see live inferences of the model running.



                    <div class="row">
                        <div class="col-md-12 text-center">
                            <img src="https://drive.google.com/uc?export=view&id=1SpqpLLYyf6JD98EgxbUcL4opvxHi8YYH"
                                alt="" height="400px" width="500px">
                            <p class="text-center" style="color: #888;">
                                <i>This shows an accurate prediction of the model during deployment.</i>
                            </p>
                        </div>
                    </div>
                    </p>
                </section>
                
                <section>
                    <h4 class="mb-3 text-center">Object Detection Task</h4>
                    <h5>Overview</h5>
                    <p>
                        Object detection is a computer vision task that identifies and localizes multiple objects within
                        an image or video. It
                        goes beyond image classification by generating bounding boxes and class labels for each object
                        detected. Algorithms
                        analyze the image, leveraging techniques like feature extraction and machine learning models to
                        accomplish this task.
                        Object detection finds applications in autonomous driving, surveillance, robotics, and augmented
                        reality, enabling
                        machines to recognize and locate objects in visual data for improved understanding and
                        interaction with the environment.
                        <br>
                        In this section, we will deploy two object detection models, one on the Arduino Nano BLE 33
                        Sense and another on the
                        Open MV Cam H7. Specifically, we will implement a bottle counter model on the Arduino Nano BLE
                        33 Sense.

                    <h5>Requirements</h5>
                    <ol>
                        <li>Arduino Nano BLE 33 Sense</li>
                        <li>Arduino IDE</li>
                    </ol>
                    </p>
                    <h4 class="text-center">Bottle Detector with Arduino Nano BLE 33 Sense</h4>
                    <p>
                        An object detection model specifically designed for the Arduino Nano BLE 33 Sense. Its purpose
                        is to identify whether an
                        object in the camera's field of view is a bottle or not. The model is provided as an Arduino
                        library, which includes all
                        the necessary parameters for performing inference on the Arduino Nano. By utilizing this model,
                        developers can easily
                        integrate bottle detection capabilities into their projects, leveraging the power and
                        versatility of the Arduino Nano
                        BLE 33 Sense for real-time object recognition

                        To set up the Bottle Detector model on the Arduino Nano BLE 33 Sense using the Arduino IDE,
                        follow these steps:
                    <ul>
                        <li>Launch the Arduino IDE on your computer.</li>
                        <li>
                            Go to the "Sketch" menu in the Arduino IDE and select <code> "Include Library" </code> >
                            <code>"Add .ZIP
                            Library..."</code>.
                        </li>
                        <li>
                            Navigate to the base folder of the <code>dsail-tech4wildlife repository</code> and select
                            the ZIP library
                            file.
                        </li>
                        <li>
                            The library will be added to the Arduino IDE, and you will be able to access its examples.
                        </li>
                        <li>
                            To access the examples, go to <code>"File"</code> > <code>"Examples"</code>
                            ><code> "bottles_inferencing"</code> >
                            <code> "nano_ble33_sense" </code>><code>"nano_ble33_sense_camera"</code>.
                        </li>
                        <li>
                            Ensure that the correct board and port are selected for the Arduino Nano 33 BLE in the
                            Arduino IDE.
                        </li>
                        <li>
                            Go to <code>"Tools" "Board" "Arduino Nano 33 BLE"</code> to
                            select the appropriate board.
                        </li>
                        <li>
                            Choose the correct port for the board by going to <code>"Tools"</code> > <code>"Port"</code>
                            and selecting the
                            appropriate port from the list.
                        </li>
                        <li>
                            Upload the code to the Arduino Nano by clicking on the <code>"Upload"</code> button in the
                            Arduino IDE.
                        </li>
                        <li>
                            Once the upload is complete, open the serial monitor in the Arduino IDE.
                            In the serial monitor, you will be able to view the inferencing results and see if the
                            detected objects are identified
                            as bottles or not.

                        </li>
                    </ul>

                    <div class="row ">
                        <div class="col-md-12 text-center">
                            <img src="https://drive.google.com/uc?export=view&id=17Jw-Km5gEaoYAMYJvdCSKLDjeZKoP_S5"
                                alt="" height="300px" width="400px">
                            <p class="text-center" style="color: #888;">
                                <i>Serial Monitor showing live inference</i>
                            </p>
                        </div>
                    </div>
                    </p>
                    <p>
                    <h4 class="text-center">Face Detection with Open MV Cam H7</h4>
                    <h5>Overview</h5>
                    Face detection is a computer vision technique used to identify and locate human faces within an
                    image or video. One
                    common approach to face detection is using the Haar cascade classifier, which is a machine
                    learning-based algorithm. The
                    Haar cascade filter uses a series of rectangular features to detect patterns resembling facial
                    features.
                    <br>
                    An example of an out-of-the-box system utilizing face detection with the Haar cascade filter is
                    available with the
                    OpenMV Cam H7. The OpenMV Cam H7 provides a pre-built face detection system that leverages the Haar
                    cascade classifier.
                    This system is ready to use and does not require additional training or configuration.
                    <br>
                    With the OpenMV Cam H7 and its built-in face detection system, developers can easily detect and
                    localize human faces in
                    real time, making it suitable for applications like facial recognition, emotion detection, and
                    human-computer
                    interaction. The Haar cascade filter implementation simplifies the process of face detection,
                    enabling quick deployment
                    and integration into various projects.
                    <br>
                    Deploying an out-of-the-box face detection to the Open MV Cam H7:
                    <ol>
                        <li>Navigate to the base folder of the <code> dsail-tech4wildlife</code> repository.</li>
                        <li>Locate the <code>open-mv-cam-h7</code> folder within the base folder.</li>
                        <li>In the<code> open-mv-cam-h7</code> folder, find the <code>main.py</code> file.</li>
                        <li>Connect the Open MV Cam to your computer using a USB cable.
                        <li>Access the USB <code>Drive D</code>, which represents the storage of the Open MV Cam.</li>
                        <li> Copy the <code>main.py</code> file from the <code>open-mv-cam-h7</code> folder.</li>
                        <li>Paste the <code>main.py</code> file into the USB Drive D.</li>
                    </ol>
                    <div class="row">
                        <div class="col-md-12 text-center">
                            <img src="https://drive.google.com/uc?export=view&id=1mTP5t0XPQJyUUS-6mtn7fd1Wt2r8eJHK" alt="" height="auto"
                                width="auto">
                            <p class="text-center" style="color: #888;">
                                <i>Replace the main file with the file from dsail-tech4wildlife</i>
                            </p>
                        </div>
                    </div>
                    To refresh the Open MV Cam, follow these steps:
                    <ul>
                        <li>Unplug the Open MV Cam from its current power source or disconnect the USB cable from the
                            computer.</li>
                        <li>If using a power supply, connect the appropriate power source to the Open MV Cam. Ensure
                            that the power supply is
                            compatible with the Open MV Cam's requirements. <br>
                            Alternatively, you can reconnect the USB cable to the Open MV Cam.
                        </li>
                        <li>Once the Open MV Cam is powered on, it will start in an idle state.
                        </li>
                    </ul>

                    <div class="row">
                        <div class="col-md-6 text-center">
                            <img src="https://drive.google.com/uc?export=view&id=1iONlilSPaPwC4jbRgaoVdgWKzBaO7LZR" alt="" height="250px"
                                width="200px">
                            <p class="text-center" style="color: #888;">
                                <i>In the idle state, without a face in the frame, the onboard LED of the Open MV Cam
                                    will appear blue.</i>
                            </p>
                        </div>
                        <div class="col-md-6 text-center">
                            <img src="https://drive.google.com/uc?export=view&id=1jO_cEaskP0vfWmfkCopxTGEJzNiv7qjS" alt="" height="300px"
                                width="400px">
                            <p class="text-center" style="color: #888;">
                                <i>When the Open MV Cam detects a face in the frame, the onboard LED will change to red
                                    to indicate the presence of a
                                    detected face.
                                </i>
                            </p>
                        </div>
                    </div>
                    </p>
                </section>

            </div>
        </div>
    </div>
    <hr>
    <div class="row">
        <div class="col-md-4 ">
            <a class="float-left" href="setup.html">
                <<< Previous</a>
        </div>
        <div class="col-md-4">
            <a class="ml-5" href="index.html">Back Home</a>
        </div>
        <div class="col-md-4">
            <a class="float-right" href="object_detection.html">Next >>></a>
        </div>
    </div>
    <hr>

    <section class="mb-3">
        <footer>

            <div class="row mt-3 mb-3">
                <div class="col-md-12 text-center">
                    DSAIL &copy; <span class="year"></span>
                </div>
            </div>
        </footer>
    </section>
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <script>

        /*GET CURRENT YEAR*/
        const date = new Date();
        document.querySelector(".year").innerHTML = date.getFullYear();

        setTimeout(function () {
            $("#message").fadeOut("slow");
        }, 3000);
    </script>
    </script>
</body>

</html>